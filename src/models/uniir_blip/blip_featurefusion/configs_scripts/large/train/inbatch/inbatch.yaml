# Train BLIPFeatureFusion model on MBEIR dataset
# <-- Important! Change this for each experiment.
experiment:
    instruct_status: "Instruct"
    exp_name: "InBatch"
    description: "${model.name} ${model.size} ${experiment.instruct_status} ${experiment.exp_name}"
    path_suffix: "${model.short_name}/${model.size}/${experiment.instruct_status}/${experiment.exp_name}/"

# WandB settings
wandb_config:
    enabled: True
    experiment_name: "${experiment.description}"

# Logger settings
logger_config:
    logger_out_dir: "logger/${experiment.path_suffix}"  # logger will be saved to uniir_dir/logger/experiment.path_suffix
    logger_out_file_name: "train.log"  #TODO: add date to log file name

# Dataset settings
data_config:
    image_size: 224, 224
    hard_neg_num: 0
    in_batch_neg_num: 0  # TODO: MOVE THIS TO MODEL Config
    shuffle_cand: True
    
    ### 返回哈希化的 passage/doc id，便于缓存与去重。
    returns:
      hashed_p_did: True

    # Relative to mbeir_data_dir
    ### query_instruct_path：指令模板 TSV 路径（通常含多种指令版本）。
    ### •	train_* / val_*：训练/验证的查询与候选池路径（JSONL 结构）。
    enable_query_instruct: True
    query_instruct_path: "instructions/query_instructions.tsv"

    train_query_data_path: "query/union_train/mbeir_union_up_train.jsonl"
    train_cand_pool_path: "cand_pool/global/mbeir_union_train_cand_pool.jsonl"

    val_query_data_path: "query/union_val/mbeir_union_val.jsonl"
    val_cand_pool_path: "cand_pool/global/mbeir_union_val_cand_pool.jsonl"

# DataLoader settings
dataloader_config:
    num_workers: 5
    train_batch_size: 115   # 79051MiB / 81559MiB
    valid_batch_size: 1035  # 57960/8/9

# Trainer settings
trainer_config:
    gradient_accumulation_steps: 1
    num_train_epochs: 20
    init_lr: 1e-5
    print_freq: 50
    weight_decay: 0.05

# Evaluator settings
evaluator:
    enable_eval: False
    eval_freq: 1
    print_freq: 10

# Model settings
model:
    name: "BLIPFeatureFusion"
    short_name: "BLIP_FF"
    size: "Large"

    vit: 'large'
    ### 开启梯度检查点，显著节省显存（以算力换显存），训练略变慢。
    vit_grad_ckpt: True
    ### 对 ViT 前 12 层或指定段落启用检查点（具体取决于实现）。可微调该值来平衡显存/速度。
    vit_ckpt_layer: 12  # Default

    embed_dim: 768
    image_size: 224
    ### 负样本记忆库大小。
	# •	需要远大于全局 batch，以保证负样本的新鲜度与多样性。
	# •	安全设定建议：queue_size ≥ 8 × (train_batch_size × NPROC)；并保证能被实现中的dequeue/enqueue步长整除（若代码有此约束）。
	# •	57,960 看起来与验证集规模或某预设常量相关；可根据GPU 数与 batch调整：如从 8×global_batch 起步，逐步增大至显存/显存外张量允许的上限。
    queue_size: 57960 # <-- Important! You need to adjust this value based on your num_gpus and batchsize
    alpha: 0.4
    tokenizer_max_length: 100

    ckpt_config:
        ckpt_dir: "checkpoint/${experiment.path_suffix}" # ckpt will be saved to uniir_dir/checkpoint/experiment.path_suffix
        pretrained_blip_url: "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large.pth"
        ckpt_name: ""
        resume_training: False


# Random seed
seed: 2023

# Distributed training settings
dist_config:
    dist_url: "env://"